{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Getting started with tomsup\n","\n","Tomsup, Theory of Mind Simulation using Python, is a Python Package for Agent Based simulations. It includes 1) a framework for running agent based simulations using 2 by 2 payoffmatrices and most notably 2) an implementation of game theory of mind in a agent based framework following the implementation of [Devaine, et al. (2017)](http://dx.plos.org/10.1371/journal.pcbi.1005833).\n","\n","This tutorial will simply introduce the framework of tomsup, for an introduction to the theory of mind (ToM) agent, see *introduction_to_tom.ipynb*.\n","\n","It is also possible for create your own agents for an introduction on this see *creating_an_agent.ipynb*.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#assuming you are in the github folder change the path - not relevant if tomsup is installed via. pip\n","import os\n","os.chdir(\"..\") # go out of the tutorials folder"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import tomsup as ts"]},{"cell_type":"markdown","metadata":{},"source":["To get an overview of possible "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":"{'RB': {'name': 'Random Bias',\n  'shorthand': 'RB',\n  'example': 'RB(bias = 0.7)',\n  'reference': 'Devaine, et al. (2017)',\n  'strategy': 'Chooses 1 randomly based on a probability or bias'},\n 'WSLS': {'name': 'Win-stay, lose-switch',\n  'shorthand': 'WSLS',\n  'example': 'WSLS()',\n  'reference': 'Nowak & Sigmund (1993)',\n  'strategy': 'If it win it chooses the same option again, if it lose it change to another'},\n 'TFT': {'name': 'Tit-for-Tat',\n  'shorthand': 'TFT',\n  'example': 'TFT()',\n  'reference': 'Shelling (1981)',\n  'strategy': 'Intended the prisoners dilemma. It starts out cooperating and then simply copies it opponents action.'},\n 'QL': {'name': 'Q-Learning Model',\n  'shorthand': 'QL',\n  'example': 'QL(learning_rate = 0.5, b_temp = 1)',\n  'reference': 'Watkinns (1992)',\n  'strategy': 'A simple reinforcement learning model, which is more choose e.g. 1 if 1 have previously been shown to yield positive result.'},\n 'TOM': {'name': 'Theory of Mind',\n  'shorthand': 'TOM',\n  'example': 'TOM(level = 2)',\n  'reference': 'Devaine, et al. (2017)',\n  'strategy': 'Recursively estimated its opponent choice probability and model parameters.'}}"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["ts.valid_agents()"]},{"cell_type":"markdown","metadata":{},"source":["---\n","## 1) Creating an agent\n","First we will set up a Random bias (RB) agent. This agent simply choses randomly with a given bias.\n","There is two ways to setup an agent, either using the agent class (e.g. RB) or using the ```create_agents()``` function. We will start by calling the agent class RB directy. For a whole list of valid agent use the ```ts.valid_agents()```."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"jung is an class of type: <class 'tomsup.agent.RB'>\nbut jung is also of has the parent class ts.Agent\njung chose 1 and his probability for choosing 1 was 0.8.\n"}],"source":["jung = ts.RB(bias = 0.8, save_history = True) #calling the agent subclass RB - for more on save_history see '3) inspecting Agent and AgentGroup'\n","\n","# Let's examine the charlie\n","print(f\"jung is an class of type: {type(jung)}\")\n","if isinstance(jung, ts.Agent):\n","    print(f\"but jung is also of has the parent class ts.Agent\")\n","\n","# let us have charlie make a choice \n","choice = jung.compete()\n","\n","print(f\"jung chose {choice} and his probability for choosing 1 was {jung.get_bias()}.\")"]},{"cell_type":"markdown","metadata":{},"source":["As previously mentioned you can also create agents using the create_agent function. Here we will create skinner as a Q-learning agent, which is a simple reinforcement learning agent, see Watkinns (1992) for more."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["skinner = ts.create_agents(agents = \"QL\", start_params = {'save_history': True}) # create a reinforcement learning agent"]},{"cell_type":"markdown","metadata":{},"source":["Since skinner is a reinforcement learning agent his compete function requires him to know which game he is playing, so he can choose based on payoff. He also needs to know his opponents move during their last turn, so that he can update his belief about his opponent choices.\n","\n","Let us have jung and skinner play the the matching pennies game. We can fetch the game from the function `PayoffMatrix`,"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<Class PayoffMatrix, Name = penny_competitive> \nThe payoff matrix of agent 0\n       |  Choice agent 1\n       |  - |  0 |  1 |\nChoice |  0 |  1 | -1 |\nagent 0|  1 | -1 |  1 |\n-----------------------\nThe payoff matrix of agent 1\n       |  Choice agent 1\n       |  - |  0 |  1 |\nChoice |  0 | -1 |  1 |\nagent 0|  1 |  1 | -1 |\n-----------------------\n[[[ 1 -1]\n  [-1  1]]\n\n [[-1  1]\n  [ 1 -1]]]\n"}],"source":["penny = ts.PayoffMatrix(name = \"penny_competitive\") # fetch the competitive matching pennies game.\n","\n","#print the payoff matrix\n","print(penny)\n","\n","#fetch the underlying numpy matrix\n","print(penny.get_matrix())"]},{"cell_type":"markdown","execution_count":24,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"unexpected EOF while parsing (<ipython-input-24-1e5fcbb1d7fe>, line 1)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-1e5fcbb1d7fe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    len(str(penny().min())\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}],"source":["Let us try to have skinner and jung compete in the matching pennies game:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"jung chose 1 and skinner chose 1, which results in a payoff for jung of 1 and skinner of -1.\n"}],"source":["jung_a = jung.compete() # a for action\n","skinner_a = skinner.compete(p_matrix = penny, agent = 1, op_choice = None) #Note that op_choice can be unspecified (or None) in the first round\n","\n","jung_p = penny.payoff(action_agent0 = jung_a, action_agent1 = skinner_a, agent = 0)\n","skinner_p = penny.payoff(action_agent0 = jung_a, action_agent1 = skinner_a, agent = 1)\n","\n","print(f\"jung chose {jung_a} and skinner chose {skinner_a}, which results in a payoff for jung of {jung_p} and skinner of {skinner_p}.\")\n","# Note that you might get different results simply by chance"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["---\n","## 2) Running a tournament\n","In the above case we saw how to have two agents compete for a single round. It is however rare that we only need 1 round and while the above functionality can be wrapped within a for we have made a `compete()` function for convenience. In this section we will also examine the class `AgentGroup`, which allows you to run tournaments with multiple agents.\n","\n","Let us start with having the two agent compete for 30 rounds in the matching pennies game:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\njung seemed to get a total of -30 points, while skinner got a total of 30.\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>round</th>\n      <th>choice_agent0</th>\n      <th>choice_agent1</th>\n      <th>payoff_agent0</th>\n      <th>payoff_agent1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   round  choice_agent0  choice_agent1  payoff_agent0  payoff_agent1\n0      0              1              1              1             -1\n1      1              1              0             -1              1\n2      2              1              0             -1              1\n3      3              0              0              1             -1\n4      4              0              0              1             -1"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["results = ts.compete(jung, skinner, p_matrix = penny, n_rounds = 50)\n","print(type(results))\n","\n","jung_sum = results['payoff_agent0'].sum()\n","skinner_sum = results['payoff_agent1'].sum()\n","\n","print(f\"jung seemed to get a total of {jung_sum} points, while skinner got a total of {skinner_sum}.\")\n","\n","results.head() #inspect the first 5 rows of the df"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["We see that the output of the compete function if a pandas dataframe. It is possible to change this to a list by specifying `return_val = \"list\"`, but having it as a dataframe allow for convenient operators attributes such as mean() and sum().\n","\n","The above case a the simplest possible version of the `compete()` function. You can also specify number of simulations, whether the agent should be reset after each simulation (this is recommended) and whether it should print what simulation it is running (`silent`).\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Running simulation 1 out of 10\n\tRunning simulation 2 out of 10\n\tRunning simulation 3 out of 10\n\tRunning simulation 4 out of 10\n\tRunning simulation 5 out of 10\n\tRunning simulation 6 out of 10\n\tRunning simulation 7 out of 10\n\tRunning simulation 8 out of 10\n\tRunning simulation 9 out of 10\n\tRunning simulation 10 out of 10\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_sim</th>\n      <th>round</th>\n      <th>choice_agent0</th>\n      <th>choice_agent1</th>\n      <th>payoff_agent0</th>\n      <th>payoff_agent1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   n_sim  round  choice_agent0  choice_agent1  payoff_agent0  payoff_agent1\n0      0      0              1              0             -1              1\n1      0      1              1              0             -1              1\n2      0      2              1              0             -1              1\n3      0      3              1              0             -1              1\n4      0      4              1              0             -1              1"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["results = ts.compete(jung, skinner, penny, n_rounds = 50, n_sim = 10, reset_agent = True, return_val = 'df', silent = False)\n","results.head()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":"n_sim\n0   -30\n1   -26\n2   -24\n3   -30\n4   -30\n5   -32\n6   -38\n7   -32\n8   -42\n9   -34\nName: payoff_agent0, dtype: int64"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["results.groupby('n_sim')['payoff_agent0'].sum()\n","\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["**Note** that by adding simulations the dataframe now also have a column called 'n_sim', for which simulations in which the results belongs.\n","\n","### AgentGroup\n","Now as promised, let us take a look at tournaments with multiple agents. We will start of by creating a group of agents using a list of the desired agents as well as a list of their starting parameters. If you are in doubt how to specify these you can always the starting parameters from an existing agent using `jung.get_start_params()`."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"{'RB_0': <tomsup.agent.RB object at 0x123667a58>, '1-tom_0': <tomsup.agent.TOM object at 0x123667a90>, 'WSLS_0': <tomsup.agent.WSLS object at 0x123667b00>, '2-tom_0': <tomsup.agent.TOM object at 0x123667b38>, 'QL_0': <tomsup.agent.QL object at 0x123667ba8>, '0-tom_0': <tomsup.agent.TOM object at 0x123667be0>}\n\n----\n\nCurrently the pair, ('RB_0', '1-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('RB_0', 'WSLS_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('RB_0', '2-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('RB_0', 'QL_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('RB_0', '0-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('1-tom_0', 'WSLS_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n/Users/au561649/Desktop/Github/tomsup/python package/tomsup/ktom_functions.py:373: Warning: Choice probability constrained at lower bound 0.001 to avoid rounding errors\n  warn(\"Choice probability constrained at lower bound 0.001 to avoid rounding errors\", Warning)\n/Users/au561649/Desktop/Github/tomsup/python package/tomsup/ktom_functions.py:370: Warning: Choice probability constrained at upper bound 0.999 to avoid rounding errors\n  warn(\"Choice probability constrained at upper bound 0.999 to avoid rounding errors\", Warning)\n\tRunning simulation 5 out of 5\nCurrently the pair, ('1-tom_0', '2-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('1-tom_0', 'QL_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('1-tom_0', '0-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('WSLS_0', '2-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('WSLS_0', 'QL_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('WSLS_0', '0-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('2-tom_0', 'QL_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('2-tom_0', '0-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n/Users/au561649/Desktop/Github/tomsup/python package/tomsup/ktom_functions.py:33: RuntimeWarning: overflow encountered in exp\n  y = epsilon + (1 - 2 * epsilon) / (1 + np.exp(-x))\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nCurrently the pair, ('QL_0', '0-tom_0'), is competing for 5 simulations, each containg 100 rounds.\n\tRunning simulation 1 out of 5\n\tRunning simulation 2 out of 5\n\tRunning simulation 3 out of 5\n\tRunning simulation 4 out of 5\n\tRunning simulation 5 out of 5\nSimulation complete\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_sim</th>\n      <th>round</th>\n      <th>choice_agent0</th>\n      <th>choice_agent1</th>\n      <th>payoff_agent0</th>\n      <th>payoff_agent1</th>\n      <th>agent0</th>\n      <th>agent1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>RB_0</td>\n      <td>1-tom_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>RB_0</td>\n      <td>1-tom_0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>RB_0</td>\n      <td>1-tom_0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>RB_0</td>\n      <td>1-tom_0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>RB_0</td>\n      <td>1-tom_0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   n_sim  round  choice_agent0  choice_agent1  payoff_agent0  payoff_agent1  \\\n0      0      0              0              0              1             -1   \n1      0      1              1              1              1             -1   \n2      0      2              1              0             -1              1   \n3      0      3              1              1              1             -1   \n4      0      4              1              0             -1              1   \n\n  agent0   agent1  \n0   RB_0  1-tom_0  \n1   RB_0  1-tom_0  \n2   RB_0  1-tom_0  \n3   RB_0  1-tom_0  \n4   RB_0  1-tom_0  "},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["agents = ['RB', 'WSLS', 'QL', '0-tom', '1-tom', '2-tom'] # create a list of agents\n","start_params = [{'bias': 0.8}, {}, {}, {}, {}, {}] # create a list of their starting parameters (an empty dictionary {} simply assumes defaults)\n","\n","\n","group = ts.AgentGroup(agents, start_params) # create a group of agents\n","print(group._agents)\n","print(\"\\n----\\n\") # to space out the outputs\n","\n","group.set_env(env = 'round_robin') # round_robin e.g. each agent will play against all other agents\n","\n","group.pairing\n","group._agents\n","# make them compete\n","results = group.compete(p_matrix = penny, n_rounds = 100, n_sim = 5)\n","results.head() #examine the first 5 rows in results"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":"pair                n_sim  agent0 \n1-tom_0 vs 0-tom_0  0      1-tom_0     12\n                    1      1-tom_0     20\n                    2      1-tom_0      0\n                    3      1-tom_0     20\n                    4      1-tom_0      4\n1-tom_0 vs 2-tom_0  0      1-tom_0    -18\n                    1      1-tom_0      6\n                    2      1-tom_0     -4\n                    3      1-tom_0      2\n                    4      1-tom_0     -6\n1-tom_0 vs QL_0     0      1-tom_0     92\n                    1      1-tom_0     90\n                    2      1-tom_0     94\n                    3      1-tom_0     98\n                    4      1-tom_0     96\n1-tom_0 vs WSLS_0   0      1-tom_0     98\n                    1      1-tom_0     96\n                    2      1-tom_0     90\n                    3      1-tom_0     94\n                    4      1-tom_0     94\n2-tom_0 vs 0-tom_0  0      2-tom_0     12\n                    1      2-tom_0      6\n                    2      2-tom_0     14\n                    3      2-tom_0     14\n                    4      2-tom_0     14\n2-tom_0 vs QL_0     0      2-tom_0     86\n                    1      2-tom_0     94\n                    2      2-tom_0     90\n                    3      2-tom_0     86\n                    4      2-tom_0     90\n                                     ... \nRB_0 vs 2-tom_0     0      RB_0       -28\n                    1      RB_0       -50\n                    2      RB_0       -44\n                    3      RB_0       -38\n                    4      RB_0       -52\nRB_0 vs QL_0        0      RB_0       -62\n                    1      RB_0       -72\n                    2      RB_0       -72\n                    3      RB_0       -42\n                    4      RB_0       -54\nRB_0 vs WSLS_0      0      RB_0       -44\n                    1      RB_0       -36\n                    2      RB_0       -38\n                    3      RB_0       -52\n                    4      RB_0       -46\nWSLS_0 vs 0-tom_0   0      WSLS_0      10\n                    1      WSLS_0      18\n                    2      WSLS_0      18\n                    3      WSLS_0      26\n                    4      WSLS_0      18\nWSLS_0 vs 2-tom_0   0      WSLS_0     -94\n                    1      WSLS_0     -90\n                    2      WSLS_0     -96\n                    3      WSLS_0     -96\n                    4      WSLS_0     -92\nWSLS_0 vs QL_0      0      WSLS_0     100\n                    1      WSLS_0     100\n                    2      WSLS_0     100\n                    3      WSLS_0     100\n                    4      WSLS_0      98\nName: payoff_agent0, Length: 75, dtype: int64"},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["results['pair'] = results[['agent0', 'agent1']].apply(lambda x: ' vs '.join(x), axis=1)\n","results.groupby(['pair', 'n_sim', 'agent0'])['payoff_agent0'].sum()"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"text/plain":"['2-tom_0', '0-tom_0', 'QL_0', '1-tom_0', 'RB_0']"},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["agent_names = [agent + '_' + str(idx) for agent in set(agents) for idx in range(agents.count(agent))]\n","agent_names"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"['RB', 'QL', '0-tom', '1-tom', '2-tom']\n['2-tom_0', '0-tom_0', 'QL_0', '1-tom_0', 'RB_0']\n"}],"source":["list(zip(agent_names, agents, start_params))\n","#_agents = {name: Agent(agent, **param) for name, agent, param in zip(agent_names, agents, start_params)}\n","print(agents)\n","print(agent_names)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"data":{"text/plain":"{'2-tom_0': <tomsup.agent.TOM at 0x12c5b8748>,\n '0-tom_0': <tomsup.agent.TOM at 0x12c5b86a0>,\n 'QL_0': <tomsup.agent.QL at 0x12cf24b00>,\n '1-tom_0': <tomsup.agent.TOM at 0x12cf24e48>,\n 'RB_0': <tomsup.agent.RB at 0x12cf24f28>}"},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["{name: ts.Agent(name.split('_')[0], **param) for name, param in zip(agent_names, start_params)}\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["As you can see once the group is created and environment it is easy to have the agent compete with one another.\n","\n","(for more possible environment, see `help(group.set_env)`)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","## 3) Inspecting Agent and AgentGroup\n","So let's examine some of the attributes of the agents, which applies to all agents. In this section we will also look a bit into how to extract an agent from an agentGroup and how to examine the enviroment."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"This is the starting parameters of jung:  {'bias': 0.7, 'var': 0, 'save_history': True}\nThis is the starting parameters of skinner:  {'learning_rate': 0.5, 'b_temp': 0.001, 'expec_val': [0.5, 0.25], 'save_history': True}\nThis is jung's last choice:  0\nThis is skinner's last choice:  0\njung's strategy is:  RB\nskinner's strategy is:  QL\n"}],"source":["# What if I want to know the starting parameters?\n","print(\"This is the starting parameters of jung: \",    jung.get_start_params()) # Note that it also prints out default parameters\n","print(\"This is the starting parameters of skinner: \", skinner.get_start_params())\n","\n","# What if I want to know the agent last choice?\n","print(\"This is jung's last choice: \",    jung.get_choice())\n","print(\"This is skinner's last choice: \", skinner.get_choice())\n","\n","# What if I want to know the agents strategy?\n","print(\"jung's strategy is: \", jung.get_strategy())\n","print(\"skinner's strategy is: \", skinner.get_strategy())"]},{"cell_type":"markdown","metadata":{},"source":["We can also get the history, recall that we specified `get_history = True` for skinner (and jung). This means we can go back and see all his previous state for the Random bias (RB) this only include the choice. This is by default returned as a dataframe. Note that by default `get_history` is `False` to save memory."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"choice\n0       1\n1       1\n2       1\n3       1\n4       0\n\n --- \n\n    choice  expected_value0  expected_value1\n0      0.0              0.5             0.25\n1      0.0              0.5             0.25\n2      0.0              0.5             0.25\n3      0.0              0.5             0.25\n4      0.0              0.5             0.25\n5      0.0              0.5             0.25\n6      0.0              0.5             0.25\n7      0.0              0.5             0.25\n8      0.0              0.5             0.25\n9      0.0              0.5             0.25\n10     0.0              0.5             0.25\n11     0.0              0.5             0.25\n12     0.0              0.5             0.25\n13     0.0              0.5             0.25\n14     0.0              0.5             0.25\n"}],"source":["# What is the history of skinner (e.g. what is his choices and internal states)\n","\n","history = jung.get_history(format = \"df\")\n","print(history.head())\n","\n","print(\"\\n --- \\n\") # for spacing\n","\n","history = skinner.get_history(format = \"df\")\n","print(history.head(15)) # the first 15 rows"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["In the above we can see the history of the two agents, both their internal states, which is none for the the random bias agent (RB) and includes two for the reinforcement learning agent (QL). The two for for the RL indicates the expected value of choosing 0 rather than 1. As expected we here see that the expected value of choosing 0 increases due to the jungs bias of 0.7.\n","\n","**Note** that if the agent are competing using multiple simulations it will resets the agent after each simulations, consequently their history is also reset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}