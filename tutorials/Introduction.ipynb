{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Getting started with tomsup\n","\n","Tomsup, Theory of Mind Simulation using Python, is a Python Package for Agent Based simulations. It includes 1) a framework for running agent based simulations using 2 by 2 payoffmatrices and most notably 2) an implementation of game theory of mind in a agent based framework following the implementation of [Devaine, et al. (2017)](http://dx.plos.org/10.1371/journal.pcbi.1005833).\n","\n","This tutorial will simply introduce the framework of tomsup, for an introduction to the theory of mind (ToM) agent, see *introduction_to_tom.ipynb*.\n","\n","It is also possible for create your own agents for an introduction on this see *creating_an_agent.ipynb*. \n","\n","Lastly we have also created an brief introduction to each of the simpler agents for this see *introduction_to_basic_agents.ipynb*.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#assuming you are in the github folder change the path - not relevant if tomsup is installed via. pip\n","import os\n","os.chdir(\"..\") # go out of the tutorials folder"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import tomsup as ts"]},{"cell_type":"markdown","metadata":{},"source":["To get an overview of possible agent you can use the following function. Give you a brief description of their strategy as well as a reference for further reading."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":"{'RB': {'name': 'Random Bias',\n  'shorthand': 'RB',\n  'example': 'RB(bias = 0.7)',\n  'reference': 'Devaine, et al. (2017)',\n  'strategy': 'Chooses 1 randomly based on a probability or bias'},\n 'WSLS': {'name': 'Win-stay, lose-switch',\n  'shorthand': 'WSLS',\n  'example': 'WSLS()',\n  'reference': 'Nowak & Sigmund (1993)',\n  'strategy': 'If it win it chooses the same option again, if it lose it change to another'},\n 'TFT': {'name': 'Tit-for-Tat',\n  'shorthand': 'TFT',\n  'example': 'TFT()',\n  'reference': 'Shelling (1981)',\n  'strategy': 'Intended the prisoners dilemma. It starts out cooperating and then simply copies it opponents action.'},\n 'QL': {'name': 'Q-Learning Model',\n  'shorthand': 'QL',\n  'example': 'QL(learning_rate = 0.5, b_temp = 1)',\n  'reference': 'Watkinns (1992)',\n  'strategy': 'A simple reinforcement learning model, which is more choose e.g. 1 if 1 have previously been shown to yield positive result.'},\n 'TOM': {'name': 'Theory of Mind',\n  'shorthand': 'TOM',\n  'example': 'TOM(level = 2)',\n  'reference': 'Devaine, et al. (2017)',\n  'strategy': 'Recursively estimated its opponent choice probability and model parameters.'}}"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["ts.valid_agents()"]},{"cell_type":"markdown","metadata":{},"source":["---\n","## 1) Creating an agent\n","First we will set up a Random bias (RB) agent. This agent simply choses randomly with a given bias.\n","There is two ways to setup an agent, either using the agent class (e.g. RB) or using the ```create_agents()``` function. We will start by calling the agent class RB directy. For a whole list of valid agent use the ```ts.valid_agents()```."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"jung is an class of type: <class 'tomsup.agent.RB'>\nbut jung is also of has the parent class ts.Agent\njung chose 0 and his probability for choosing 1 was 0.7.\n"}],"source":["jung = ts.RB(bias = 0.7, save_history = True) #calling the agent subclass RB - for more on save_history see '3) inspecting Agent and AgentGroup'\n","\n","# Let's examine the charlie\n","print(f\"jung is an class of type: {type(jung)}\")\n","if isinstance(jung, ts.Agent):\n","    print(f\"but jung is also of has the parent class ts.Agent\")\n","\n","# let us have charlie make a choice \n","choice = jung.compete()\n","\n","print(f\"jung chose {choice} and his probability for choosing 1 was {jung.get_bias()}.\")"]},{"cell_type":"markdown","metadata":{},"source":["As previously mentioned you can also create agents using the create_agent function. Here we will create skinner as a Q-learning agent, which is a simple reinforcement learning agent, see Watkinns (1992) for more."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["skinner = ts.create_agents(agents = \"QL\", start_params = {'save_history': True}) # create a reinforcement learning agent"]},{"cell_type":"markdown","metadata":{},"source":["Since skinner is a reinforcement learning agent his compete function requires him to know which game he is playing, so he can choose based on payoff. He also needs to know his opponents move during their last turn, so that he can update his belief about his opponent choices.\n","\n","Let us have jung and skinner play the the matching pennies game. We can fetch the game from the function `PayoffMatrix`,"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Help on class PayoffMatrix in module tomsup.payoffmatrix:\n\nclass PayoffMatrix(builtins.object)\n |  PayoffMatrix(name, predefined=None)\n |  \n |  A class of 2 by 2 payoff matrices.\n |  \n |  Currently include the following games:\n |  The staghunt game: \n |      'staghunt'\n |  The matching pennies game (coop and competive): \n |      'penny_competive'\n |      'penny_cooperative'\n |  The party dilemma:\n |      'party'\n |  The Battle of the sexes:\n |      'sexes'\n |  The chicken game:\n |      'chicken'\n |  The deadlock:\n |      'deadlock'\n |  \n |  Example:\n |  >>> staghunt = PayoffMatrix(name=\"staghunt\")\n |  >>> staghunt.payoff(action_agent0 = 1, action_agent1 = 1 , agent = 0)\n |  5\n |  >>> staghunt.payoff(action_agent0 = 1, action_agent1 = 0 , agent = 0)\n |  0\n |  >>> staghunt.payoff(action_agent0 = 0, action_agent1 = 1 , agent = 0)\n |  3\n |  >>> chicken = PayoffMatrix(name=\"chicken\")\n |  >>> chicken.payoff(0, 1 , 0)\n |  -1\n |  >>> dead = PayoffMatrix(name=\"deadlock\")\n |  >>> dead.payoff(1, 0, 1)\n |  0\n |  >>> sexes = PayoffMatrix(name=\"sexes\")\n |  >>> sexes.payoff(1, 1, 0)\n |  5\n |  \n |  Methods defined here:\n |  \n |  __call__(self)\n |      Call self as a function.\n |  \n |  __init__(self, name, predefined=None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __str__(self)\n |      Return str(self).\n |  \n |  get_matrix(self)\n |  \n |  payoff(self, action_agent0, action_agent1, agent=0)\n |      assumes action_agent0 and action_agent1 to be integers\n |      agent is either 'p0' or 'p1' indicating whether the agent is player one or two\n |      TODO: update docstring\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n<Class PayoffMatrix, Name = penny_competitive> \nThe payoff matrix of agent 0\n       |  Choice agent 1\n       |    |  0 |  1 |\n       | ------------ |\nChoice |  0 |  1 | -1 |\nagent 0|  1 | -1 |  1 |\n \nThe payoff matrix of agent 1\n       |  Choice agent 1\n       |    |  0 |  1 |\n       | ------------ |\nChoice |  0 | -1 |  1 |\nagent 0|  1 |  1 | -1 |\n \n[[[ 1 -1]\n  [-1  1]]\n\n [[-1  1]\n  [ 1 -1]]]\n"}],"source":["help(ts.PayoffMatrix) # check which payoff matreces are implemented in tomsup\n","\n","penny = ts.PayoffMatrix(name = \"penny_competitive\") # fetch the competitive matching pennies game.\n","#It is also possible to create a custom payoff matrix by inputting the desired values into the PayoffMatrix function\n","\n","#print the payoff matrix\n","print(penny)\n","\n","#fetch the underlying numpy matrix\n","print(penny.get_matrix())"]},{"cell_type":"markdown","execution_count":24,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"unexpected EOF while parsing (<ipython-input-24-1e5fcbb1d7fe>, line 1)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-1e5fcbb1d7fe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    len(str(penny().min())\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}],"source":["Let us try to have skinner and jung compete in the matching pennies game:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"jung chose 1 and skinner chose 0, which results in a payoff for jung of -1 and skinner of 1.\n"}],"source":["jung_a = jung.compete() # a for action\n","skinner_a = skinner.compete(p_matrix = penny, agent = 1, op_choice = None) #Note that op_choice can be unspecified (or None) in the first round\n","\n","jung_p = penny.payoff(action_agent0 = jung_a, action_agent1 = skinner_a, agent = 0)\n","skinner_p = penny.payoff(action_agent0 = jung_a, action_agent1 = skinner_a, agent = 1)\n","\n","print(f\"jung chose {jung_a} and skinner chose {skinner_a}, which results in a payoff for jung of {jung_p} and skinner of {skinner_p}.\")\n","# Note that you might get different results simply by chance"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["---\n","## 2) Running a tournament\n","In the above case we saw how to have two agents compete for a single round. It is however rare that we only need 1 round and while the above functionality can be wrapped within a for we have made a `compete()` function for convenience. In this section we will also examine the class `AgentGroup`, which allows you to run tournaments with multiple agents.\n","\n","Let us start with having the two agent compete for 30 rounds in the matching pennies game:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\njung seemed to get a total of -16 points, while skinner got a total of 16.\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>round</th>\n      <th>choice_agent0</th>\n      <th>choice_agent1</th>\n      <th>payoff_agent0</th>\n      <th>payoff_agent1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   round  choice_agent0  choice_agent1  payoff_agent0  payoff_agent1\n0      0              1              0             -1              1\n1      1              1              0             -1              1\n2      2              0              1             -1              1\n3      3              0              0              1             -1\n4      4              0              1             -1              1"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["results = ts.compete(jung, skinner, p_matrix = penny, n_rounds = 30)\n","print(type(results))\n","\n","jung_sum = results['payoff_agent0'].sum()\n","skinner_sum = results['payoff_agent1'].sum()\n","\n","print(f\"jung seemed to get a total of {jung_sum} points, while skinner got a total of {skinner_sum}.\")\n","\n","results.head() #inspect the first 5 rows of the df"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["We see that the output of the compete function if a pandas dataframe. It is possible to change this to a list by specifying `return_val = \"list\"`, but having it as a dataframe allow for convenient operators attributes such as mean() and sum().\n","\n","The above case a the simplest possible version of the `compete()` function. You can also specify number of simulations, whether the agent should be reset after each simulation (this is recommended) and whether it should print what simulation it is running (`silent`).\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Running simulation 1 out of 3\n\tRunning simulation 2 out of 3\n\tRunning simulation 3 out of 3\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_sim</th>\n      <th>round</th>\n      <th>choice_agent0</th>\n      <th>choice_agent1</th>\n      <th>payoff_agent0</th>\n      <th>payoff_agent1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   n_sim  round  choice_agent0  choice_agent1  payoff_agent0  payoff_agent1\n0      0      0              1              0             -1              1\n1      0      1              0              0              1             -1\n2      0      2              1              0             -1              1\n3      0      3              1              0             -1              1\n4      0      4              1              0             -1              1"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["results = ts.compete(jung, skinner, penny, n_rounds = 30, n_sim = 3, reset_agent = True, return_val = 'df', silent = False)\n","results.head()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["**Note** that by adding simulations the dataframe now also have a column called 'n_sim', for which simulations in which the results belongs.\n","\n","### AgentGroup\n","Now as promised, let us take a look at tournaments with multiple agents. We will start of by creating a group of agents using a list of the desired agents as well as a list of their starting parameters. If you are in doubt how to specify these you can always the starting parameters from an existing agent using `jung.get_start_params()`."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<Class AgentGroup, envinment = None \n\nQL_0\t | \t{'bias': 0.7}\nRB_0\t | \t{'learning_rate': 0.5}\nWSLS_0\t | \t{}\n\n----\n\nCurrently the pair, ('QL_0', 'RB_0'), is competing for 2 simulations, each containg 20 rounds.\n\tRunning simulation 1 out of 2\n\tRunning simulation 2 out of 2\nCurrently the pair, ('QL_0', 'WSLS_0'), is competing for 2 simulations, each containg 20 rounds.\n\tRunning simulation 1 out of 2\n\tRunning simulation 2 out of 2\nCurrently the pair, ('RB_0', 'WSLS_0'), is competing for 2 simulations, each containg 20 rounds.\n\tRunning simulation 1 out of 2\n\tRunning simulation 2 out of 2\nSimulation complete\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_sim</th>\n      <th>round</th>\n      <th>choice_agent0</th>\n      <th>choice_agent1</th>\n      <th>payoff_agent0</th>\n      <th>payoff_agent1</th>\n      <th>agent0</th>\n      <th>agent1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>QL_0</td>\n      <td>RB_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>QL_0</td>\n      <td>RB_0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>QL_0</td>\n      <td>RB_0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>QL_0</td>\n      <td>RB_0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>QL_0</td>\n      <td>RB_0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   n_sim  round  choice_agent0  choice_agent1  payoff_agent0  payoff_agent1  \\\n0      0      0              0              1             -1              1   \n1      0      1              0              1             -1              1   \n2      0      2              0              0              1             -1   \n3      0      3              0              0              1             -1   \n4      0      4              0              0              1             -1   \n\n  agent0 agent1  \n0   QL_0   RB_0  \n1   QL_0   RB_0  \n2   QL_0   RB_0  \n3   QL_0   RB_0  \n4   QL_0   RB_0  "},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["agents = ['RB', 'QL', 'WSLS'] # create a list of agents\n","start_params = [{'bias': 0.7}, {'learning_rate': 0.5}, {}] # create a list of their starting parameters (an empty dictionary {} simply assumes defaults)\n","\n","group = ts.create_agents(agents, start_params) # create a group of agents\n","print(group)\n","print(\"\\n----\\n\") # to space out the outputs\n","\n","group.set_env(env = 'round_robin') # round_robin e.g. each agent will play against all other agents\n","\n","# make them compete\n","results = group.compete(p_matrix = penny, n_rounds = 20, n_sim = 2)\n","results.head() #examine the first 5 rows in results"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["As you can see once the group is created and environment it is easy to have the agent compete with one another.\n","\n","(for more possible environment, see `help(group.set_env)`)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","## 3) Inspecting Agent and AgentGroup\n","So let's examine some of the attributes of the agents, which applies to all agents. In this section we will also look a bit into how to extract an agent from an agentGroup and how to examine the enviroment."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# What if I want to know the starting parameters?\n","print(\"This is the starting parameters of jung: \",    jung.get_start_params()) # Note that it also prints out default parameters\n","print(\"This is the starting parameters of skinner: \", skinner.get_start_params())\n","\n","# What if I want to know the agent last choice?\n","print(\"This is jung's last choice: \",    jung.get_choice())\n","print(\"This is skinner's last choice: \", skinner.get_choice())\n","\n","# What if I want to know the agents strategy?\n","print(\"jung's strategy is: \", jung.get_strategy())\n","print(\"skinner's strategy is: \", skinner.get_strategy())"]},{"cell_type":"markdown","metadata":{},"source":["We can also get the history, recall that we specified `get_history = True` for skinner (and jung). This means we can go back and see all his previous state for the Random bias (RB) this only include the choice. This is by default returned as a dataframe. Note that by default `get_history` is `False` to save memory."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# What is the history of skinner (e.g. what is his choices and internal states)\n","\n","history = jung.get_history(format = \"df\")\n","print(history.head())\n","\n","print(\"\\n --- \\n\") # for spacing\n","\n","history = skinner.get_history(format = \"df\")\n","print(history.head(15)) # the first 15 rows"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["In the above we can see the history of the two agents, both their internal states, which is none for the the random bias agent (RB) and includes two for the reinforcement learning agent (QL). The two for for the RL indicates the expected value of choosing 0 rather than 1. As expected we here see that the expected value of choosing 0 increases due to the jungs bias of 0.7.\n","\n","**Note** that if the agent are competing using multiple simulations it will resets the agent after each simulations, consequently their history is also reset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python37364bit7b7515eaeb334d758542b291d6ab5056","display_name":"Python 3.7.3 64-bit"}}}